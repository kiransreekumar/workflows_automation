variables:
  - group: 'Devops Automation'


trigger:
  branches:
    include:
    - main


# Specify the operating system for the agent that runs on the Azure virtual
# machine for the build pipeline (known as the build agent). The virtual
# machine image in this example uses the Ubuntu 22.04 virtual machine
# image in the Azure Pipeline agent pool. See
# https://learn.microsoft.com/azure/devops/pipelines/agents/hosted#software
pool:
  vmImage: ubuntu-22.04

stages:
- stage: 'Git_and_Bundles'

  jobs:
  - job: 'Deploy_Bundles'
    pool:
      vmImage: 'ubuntu-20.04'


# Download the files from the designated branch in the remote Git repository
# onto the build agent.

    steps:
    - checkout: self
      persistCredentials: true
      clean: true

# Generate the deployment artifact. To do this, the build agent gathers
# all the new or updated code to be given to the release pipeline,
# including the sample Python code, the Python notebooks,
# the Python wheel library component files, and the related Databricks asset
# bundle settings.
# Use git diff to flag files that were added in the most recent Git merge.
# Then add the files to be used by the release pipeline.
# The implementation in your pipeline will likely be different.
# The objective here is to add all files intended for the current release.
    - script: |
        git diff --name-only --diff-filter=AMR HEAD^1 HEAD | xargs -I '{}' cp --parents -r '{}' $(Build.BinariesDirectory)
        mkdir -p $(Build.BinariesDirectory)/src/dabwhl
        cp $(Build.Repository.LocalPath)/dist/*.* $(Build.BinariesDirectory)/dist
        cp $(Build.Repository.LocalPath)/*.* $(Build.BinariesDirectory)
      displayName: 'Get Changes'

# Create the deployment artifact and then publish it to the
# artifact repository.
    - task: ArchiveFiles@2
      inputs:
        rootFolderOrFile: '$(Build.BinariesDirectory)'
        includeRootFolder: false
        archiveType: 'zip'
        archiveFile: '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip'
        replaceExistingArchive: true

    - task: PublishBuildArtifacts@1
      inputs:
        ArtifactName: 'DatabricksBuild'

    - script: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
      displayName: 'Install Databricks CLI'

    - script: |
        pip install poetry
      displayName: 'Install Poetry'


    - script: databricks bundle deploy --var="warehouse_id=$(BUNDLE_VAR_warehouse_id),node_type=$(BUNDLE_VAR_node_type)" -t $(BUNDLE_TARGET)
      displayName: 'Deploying Bundles'

    - script: databricks api post /api/2.0/sql/statements --json @$(Build.Repository.LocalPath)/volume.json
      displayName: 'Create Volume'
   
    - script: databricks fs cp $(Build.Repository.LocalPath)/dist/*.* dbfs:/Volumes/main/default/wheel --overwrite
      displayName: 'Copy to volumes'
    




