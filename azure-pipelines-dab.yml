variables:
  - group: 'Devops Automation'


trigger:
  branches:
    include:
    - main


# Specify the operating system for the agent that runs on the Azure virtual
# machine for the build pipeline (known as the build agent). The virtual
# machine image in this example uses the Ubuntu 22.04 virtual machine
# image in the Azure Pipeline agent pool. See
# https://learn.microsoft.com/azure/devops/pipelines/agents/hosted#software
pool:
  vmImage: ubuntu-22.04



stages:
- stage: 'Git_and_Bundles'

  jobs:
  - job: 'Deploy_Bundles'
    pool:
      vmImage: 'ubuntu-20.04'


    steps:
    - checkout: self
      persistCredentials: true
      clean: true
# Use Python version v0
# Use the specified version of Python from the tool cache, optionally adding it to the PATH.
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.11' # string. Required. Version spec. Default: 3.x.
        architecture: 'x64' # 'x86' | 'x64'. Required. Architecture. Default: x64.

# Download the files from the designated branch in the remote Git repository
# onto the build agent.

# Generate the deployment artifact. To do this, the build agent gathers
# all the new or updated code to be given to the release pipeline,
# including the sample Python code, the Python notebooks,
# the Python wheel library component files, and the related Databricks asset
# bundle settings.
# Use git diff to flag files that were added in the most recent Git merge.
# Then add the files to be used by the release pipeline.
# The implementation in your pipeline will likely be different.
# The objective here is to add all files intended for the current release.
    - script: |
        git diff --name-only --diff-filter=AMR HEAD^1 HEAD | xargs -I '{}' cp --parents -r '{}' $(Build.BinariesDirectory)
        mkdir -p $(Build.BinariesDirectory)/src/dabwhl
        cp $(Build.Repository.LocalPath)/dist/*.* $(Build.BinariesDirectory)/dist
        cp $(Build.Repository.LocalPath)/*.* $(Build.BinariesDirectory)
      displayName: 'Get Changes'

# Create the deployment artifact and then publish it to the
# artifact repository.
    - task: ArchiveFiles@2
      inputs:
        rootFolderOrFile: '$(Build.BinariesDirectory)'
        includeRootFolder: false
        archiveType: 'zip'
        archiveFile: '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip'
        replaceExistingArchive: true

    - task: PublishBuildArtifacts@1
      inputs:
        ArtifactName: 'DatabricksBuild'

    - script: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
      displayName: 'Install Databricks CLI'

    - script: |
        pip install poetry
      displayName: 'Install Poetry'


    - script: databricks bundle deploy --var="warehouse_id=$(BUNDLE_VAR_warehouse_id),node_type=$(BUNDLE_VAR_node_type)" -t $(BUNDLE_TARGET)
      displayName: 'Deploying Bundles'

   
    - script: databricks fs cp $(Build.Repository.LocalPath)/dist/*.whl dbfs:/Volumes/wheel_catalog/wheel_schema/wheel_volume --overwrite
      displayName: 'Copy to volumes'
    




